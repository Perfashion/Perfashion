{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pickle\n",
    "import textwrap\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRINGS_ENCODING = 'ISO-8859-1'\n",
    "\n",
    "class Product:\n",
    "    def __init__(self, p_id, name, caption, image, category, subcategory, pose=\"id_gridfs_1\"):\n",
    "        self.p_id = p_id\n",
    "        self.pose = pose\n",
    "        self.name = name\n",
    "        self.caption = caption\n",
    "        self.image = image\n",
    "        self.category = category\n",
    "        self.subcategory = subcategory\n",
    "        self._image_features = None\n",
    "        self._embedding = None\n",
    "        self._caption_embedding = None\n",
    "\n",
    "    @property\n",
    "    def image_features(self):\n",
    "        return self._image_features\n",
    "\n",
    "    @image_features.setter\n",
    "    def image_features(self, value):\n",
    "        self._image_features = value\n",
    "\n",
    "    @property\n",
    "    def embedding(self):\n",
    "        return self._embedding\n",
    "\n",
    "    @embedding.setter\n",
    "    def embedding(self, value):\n",
    "        self._embedding = value\n",
    "\n",
    "    @property\n",
    "    def caption_embedding(self):\n",
    "        return self._caption_embedding\n",
    "\n",
    "    @caption_embedding.setter\n",
    "    def caption_embedding(self, value):\n",
    "        self._caption_embedding = value\n",
    "\n",
    "    def decoded_caption(self):\n",
    "        return self.caption.decode(STRINGS_ENCODING).replace('ÃÂÃÂÃÂÃÂ©', '')\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.p_id < other.p_id\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.p_id == other.p_id and self.pose == other.pose\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.p_id, self.pose))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"product_id: {self.p_id}\\nname: {self.name.decode(STRINGS_ENCODING) }\\ncaption: {self.caption.decode(STRINGS_ENCODING) }\\ncategory: {self.category.decode(STRINGS_ENCODING) } \\nsubcategory: {self.subcategory.decode(STRINGS_ENCODING) }\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"validation_dataset_w_embeddings\"\n",
    "if (not os.path.exists(dataset_folder)):\n",
    "    gdown.download(f\"https://drive.google.com/uc?id=1KcDFzv4JjuEQyyIvC7BkHgxN7LzbInPl\", f\"{dataset_folder}.tar.gz\", False)\n",
    "    !tar -xvf \"{dataset_folder}.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "validation_dataset = tf.data.experimental.load(dataset_folder, \n",
    "                                            (tf.TensorSpec(shape=(), dtype = tf.int32), #id\n",
    "                                             tf.TensorSpec(shape=(), dtype = tf.string), #name \n",
    "                                             tf.TensorSpec(shape=(), dtype = tf.string), #category\n",
    "                                             tf.TensorSpec(shape=(), dtype = tf.string), #subcategory\n",
    "                                             tf.TensorSpec(shape=(), dtype = tf.string), #caption\n",
    "                                             tf.TensorSpec(shape=(256,256, 3), dtype = tf.uint8), #image \n",
    "                                             tf.TensorSpec(shape=(131072,), dtype = tf.float32), #image features\n",
    "                                             tf.TensorSpec(shape=(768,), dtype = tf.float32), #image_embedding\n",
    "                                             tf.TensorSpec(shape=(768,), dtype = tf.float32)), #caption embedding\n",
    "                                            compression=\"GZIP\")\n",
    "\n",
    "products_dict = {} # a dictionary that maps ids to products\n",
    "for p in tqdm(tfds.as_numpy(validation_dataset)):\n",
    "    product = Product(p_id = p[0], name = p[1], caption = p[4], image = p[5], category = p[2], subcategory = p[3])\n",
    "    product.image_features = p[6]\n",
    "    product.embedding = p[7]\n",
    "    product.caption_embedding = p[8]\n",
    "    products_dict[p[0]] = product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load candidate sets: lists of elements of the form: query_id -> [(candidate_id, label), ...].\n",
    "# The relevant document has label=1, the other ones 0\n",
    "txt2img_file = \"txt2img.pkl\"\n",
    "img2txt_file = \"img2txt.pkl\"\n",
    "#download files\n",
    "if (not os.path.exists(txt2img_file)):\n",
    "    gdown.download(f\"https://drive.google.com/uc?id=101zaYhWws6CkWePEdBg5WVb9C8HXflC5\", txt2img_file, False)\n",
    "if (not os.path.exists(img2txt_file)):\n",
    "    gdown.download(f\"https://drive.google.com/uc?id=1IV61fyjUt7Pgve2t4ZuqS7XpAPw9vDLO\", img2txt_file, False)\n",
    "\n",
    "#load\n",
    "txt2img_candidate_sets = pickle.load(open(txt2img_file, \"rb\"))\n",
    "img2txt_candidate_sets = pickle.load(open(img2txt_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank@K Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x, y, axis = None):\n",
    "    return np.linalg.norm(x-y, axis = axis)\n",
    "\n",
    "def compute_rank_at_k(sorted_documents, Ks):\n",
    "    fount_at_top_k = {k:0 for k in Ks}\n",
    "    for _, documents in sorted_documents.items():\n",
    "        for i, document in enumerate(documents):\n",
    "            if document[1]: #if label is equal to 1 we found the relevant document\n",
    "                fount_at_top_k = {k:v + (1 if k>=i+1 else 0) for k, v in fount_at_top_k.items()}\n",
    "                break # we can break since there will be no other relevant document\n",
    "    return fount_at_top_k\n",
    "\n",
    "Ks = [1, 5, 10]\n",
    "\n",
    "txt2img_sorted_documents = {}\n",
    "print(\"=== Text-to-Image Retrieval ===\")\n",
    "for query_id, candidates in txt2img_candidate_sets.items():\n",
    "    caption_emb = np.array(products_dict[query_id].caption_embedding)\n",
    "    image_embs = np.array([products_dict[candidate_id].embedding for candidate_id, _ in candidates])\n",
    "    scores = np.array(euclidean_distance(caption_emb, image_embs, axis=1))\n",
    "    sorted_indexes = np.argsort(scores)\n",
    "    txt2img_sorted_documents[query_id] = list(map(candidates.__getitem__, list(sorted_indexes)))   \n",
    "rank_at_k = compute_rank_at_k(txt2img_sorted_documents, Ks)            \n",
    "for k in Ks:\n",
    "    print(f\"Rank @ {k}: {float(rank_at_k[k])/float(len(txt2img_sorted_documents) + 1e-5)}\") \n",
    "                      \n",
    "print(\"=== Image-to-Text Retrieval ===\")\n",
    "img2txt_sorted_documents = {}\n",
    "for query_id, candidates in img2txt_candidate_sets.items():\n",
    "    caption_embs = np.array([products_dict[candidate_id].caption_embedding for candidate_id, _ in candidates])\n",
    "    image_emb = np.array(products_dict[query_id].embedding)\n",
    "    scores = np.array(euclidean_distance(image_emb, caption_embs, axis=1))\n",
    "    sorted_indexes = np.argsort(scores)\n",
    "    img2txt_sorted_documents[query_id] = list(map(candidates.__getitem__, list(sorted_indexes)))    \n",
    "rank_at_k = compute_rank_at_k(img2txt_sorted_documents, Ks)\n",
    "for k in Ks:\n",
    "    print(f\"Rank @ {k}: {float(rank_at_k[k])/float(len(img2txt_sorted_documents) + 1e-5)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_product_image(p, ax, color_id):\n",
    "    image = Image.fromarray(p.image, 'RGB')\n",
    "    ax.imshow(image)\n",
    "    if (p.p_id==color_id):\n",
    "        ax.axis('on')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        for s in ax.spines:\n",
    "            ax.spines[s].set_color('green')\n",
    "            ax.spines[s].set_linewidth(3.5)\n",
    "    \n",
    "def visualize_product_caption(p, ax, color_id):\n",
    "    caption = p.decoded_caption()\n",
    "    props = dict(boxstyle='round', facecolor=\"white\")\n",
    "    if (p.p_id==color_id):\n",
    "        props[\"facecolor\"]=\"green\"\n",
    "        props[\"alpha\"] = 0.5\n",
    "    ax.text(0.1, 0.9, textwrap.fill(caption, 32), transform=ax.transAxes, fontsize=20, verticalalignment='top', bbox=props)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Retrieval Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUERIES = 5\n",
    "N_RESULTS = 5\n",
    "TASK = \"txt2img\" # change with \"img2txt\"\n",
    "SAVE_RESULT = False\n",
    "\n",
    "if TASK==\"txt2img\":\n",
    "    documents_dict = txt2img_sorted_documents\n",
    "    query_visualization_function = visualize_product_caption\n",
    "    document_visualization_function = visualize_product_image\n",
    "elif TASK==\"img2txt\":\n",
    "    documents_dict = img2txt_sorted_documents\n",
    "    query_visualization_function = visualize_product_image\n",
    "    document_visualization_function = visualize_product_caption\n",
    "\n",
    "\n",
    "for i, (query_id, documents) in zip(range(NUM_QUERIES), documents_dict.items()):\n",
    "    fig, axes = plt.subplots(ncols=N_RESULTS+1, nrows=1, constrained_layout=True, sharex=True, sharey=True, figsize=(32,32))\n",
    "    product = products_dict[query_id]\n",
    "    retrieved = [products_dict[d] for d,_ in documents]\n",
    "    for col, ax in enumerate(axes):\n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')\n",
    "        if(col==0):\n",
    "            query_visualization_function(product, ax, None)\n",
    "        else:\n",
    "            document_visualization_function(retrieved[col-1], ax, product.p_id) \n",
    "    \n",
    "    if SAVE_RESULT:\n",
    "        fig.savefig(f\"{TASK}_{i}.svg\", format=\"svg\", bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd0a60dff64f780099cf68dbf7bd1af139566231657a674d0920ab83104a484024f",
   "display_name": "Python 3.9.3  ('perfashion': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "a60dff64f780099cf68dbf7bd1af139566231657a674d0920ab83104a484024f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}