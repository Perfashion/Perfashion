{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox, TextArea\n",
    "import numpy as np\n",
    "import os\n",
    "import gdown\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRINGS_ENCODING = 'ISO-8859-1'\n",
    "\n",
    "class Product:\n",
    "    def __init__(self, p_id, name, caption, image, category, subcategory, pose=\"id_gridfs_1\"):\n",
    "        self.p_id = p_id\n",
    "        self.pose = pose\n",
    "        self.name = name\n",
    "        self.caption = caption\n",
    "        self.image = image\n",
    "        self.category = category\n",
    "        self.subcategory = subcategory\n",
    "        self._image_features = None\n",
    "        self._embedding = None\n",
    "        self._caption_embedding = None\n",
    "\n",
    "    @property\n",
    "    def image_features(self):\n",
    "        return self._image_features\n",
    "\n",
    "    @image_features.setter\n",
    "    def image_features(self, value):\n",
    "        self._image_features = value\n",
    "\n",
    "    @property\n",
    "    def embedding(self):\n",
    "        return self._embedding\n",
    "\n",
    "    @embedding.setter\n",
    "    def embedding(self, value):\n",
    "        self._embedding = value\n",
    "\n",
    "    @property\n",
    "    def caption_embedding(self):\n",
    "        return self._caption_embedding\n",
    "\n",
    "    @caption_embedding.setter\n",
    "    def caption_embedding(self, value):\n",
    "        self._caption_embedding = value\n",
    "\n",
    "    def decoded_caption(self):\n",
    "        return self.caption.decode(STRINGS_ENCODING).replace('ÃÂÃÂÃÂÃÂ©', '')\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.p_id < other.p_id\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.p_id == other.p_id and self.pose == other.pose\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.p_id, self.pose))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"product_id: {self.p_id}\\nname: {self.name.decode(STRINGS_ENCODING) }\\ncaption: {self.caption.decode(STRINGS_ENCODING) }\\ncategory: {self.category.decode(STRINGS_ENCODING) } \\nsubcategory: {self.subcategory.decode(STRINGS_ENCODING) }\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"validation_dataset_w_embeddings\"\n",
    "if (not os.path.exists(dataset_folder)):\n",
    "    gdown.download(f\"https://drive.google.com/uc?id=1KcDFzv4JjuEQyyIvC7BkHgxN7LzbInPl\", f\"{dataset_folder}.tar.gz\", False)\n",
    "    !tar -xvf \"{dataset_folder}.tar.gz\"\n",
    "    \n",
    "validation_dataset = tf.data.experimental.load(\"validation_dataset_w_embeddings\", \n",
    "                                            (tf.TensorSpec(shape=(), dtype = tf.int32), #id\n",
    "                                             tf.TensorSpec(shape=(), dtype = tf.string), #name \n",
    "                                             tf.TensorSpec(shape=(), dtype = tf.string), #category\n",
    "                                             tf.TensorSpec(shape=(), dtype = tf.string), #subcategory\n",
    "                                             tf.TensorSpec(shape=(), dtype = tf.string), #caption\n",
    "                                             tf.TensorSpec(shape=(256,256, 3), dtype = tf.uint8), #image \n",
    "                                             tf.TensorSpec(shape=(131072,), dtype = tf.float32), #image features\n",
    "                                             tf.TensorSpec(shape=(768,), dtype = tf.float32), #image_embedding\n",
    "                                             tf.TensorSpec(shape=(768,), dtype = tf.float32)), #caption embedding\n",
    "                                            compression=\"GZIP\")\n",
    "products_list = [] # a list of products\n",
    "for p in tqdm(tfds.as_numpy(validation_dataset)):\n",
    "    product = Product(p_id = p[0], name = p[1], caption = p[4], image = p[5], category = p[2], subcategory = p[3])\n",
    "    #product.image_features = p[6]\n",
    "    product.embedding = p[7]\n",
    "    product.caption_embedding = p[8]\n",
    "    products_list.append(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_ids = [91765, 108990, 88120, 1582273]\n",
    "caption_embs = [p.caption_embedding for p in products_list if p.p_id in captions_ids]\n",
    "tsne = TSNE(n_components=2,perplexity=65, early_exaggeration=12.0, n_iter=2000, random_state=42, learning_rate=200, init='pca')\n",
    "\n",
    "tsne = tsne.fit_transform(np.concatenate(([p.embedding for p in products_list], caption_embs)))"
   ]
  },
  {
   "source": [
    "\n",
    "def textscatter(x, y, queries, box_alignments, ax=None, text_size = 15):\n",
    "  if ax is None:\n",
    "    ax = plt.gca()\n",
    "    \n",
    "  artists = []\n",
    "  for x0, y0, q, box_align in zip(x, y, queries, box_alignments):\n",
    "    ax.plot(x0,y0, \".r\", markersize=120)\n",
    "    offsetbox = TextArea(f\"{textwrap.fill(q, width=42)}\", minimumdescent=False, textprops ={\"size\":text_size})\n",
    "          \n",
    "        \n",
    "    ab = AnnotationBbox(offsetbox, (x0, y0),\n",
    "                      xycoords='data',\n",
    "                      boxcoords=None,\n",
    "                      box_alignment=box_align,\n",
    "                      arrowprops=dict(arrowstyle=\"->\"))\n",
    "    ab.set_zorder(-1)\n",
    "    artists.append(ax.add_artist(ab))\n",
    "\n",
    "  ax.update_datalim(np.column_stack([x, y]))\n",
    "  ax.autoscale()\n",
    "  return artists\n",
    "\n",
    "\n",
    "def imscatter(x, y, images, ax=None, zoom=1):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    artists = []\n",
    "    for x0, y0, i in zip(x, y, images):\n",
    "        im = OffsetImage(i, zoom=zoom)\n",
    "        #x, y = np.atleast_1d(x, y)\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=False)\n",
    "        ab.set_zorder(-2)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return artists\n",
    "\n",
    "ROUND_COORDINATES = True\n",
    "fig, ax = plt.subplots(figsize=(160,100))\n",
    "plt.axis('off')\n",
    "\n",
    "num_images = len(products_list)\n",
    "if not ROUND_COORDINATES:\n",
    "    imscatter(tsne[:num_images, 0], tsne[:num_images, 1], [p.image for p in products_list], ax, zoom=0.3)\n",
    "else:\n",
    "    imscatter(list(map(round, tsne[:,0])), list(map(round, tsne[:,1])),  [p.image for p in products_list], ax, zoom=0.3)\n",
    "\n",
    "textscatter(tsne[num_images:, 0], tsne[num_images:, 1], \n",
    "            [p.decoded_caption() for p in products_list if p.p_id in captions_ids], \n",
    "            [(1.5, 1.5), (-0.3, -0.3), (1.5, 5.5), (0.5, 4)], ax, text_size=100)\n",
    "plt.show()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python393jvsc74a57bd0a60dff64f780099cf68dbf7bd1af139566231657a674d0920ab83104a484024f",
   "display_name": "Python 3.9.3  ('perfashion': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}